{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from models import get_feature_extractor\n",
    "model = get_feature_extractor('cct7_h')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "CCT(\n  (tokenizer): Tokenizer(\n    (conv_layers): Sequential(\n      (0): Sequential(\n        (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): ReLU()\n        (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      )\n    )\n    (flattener): Flatten(start_dim=2, end_dim=3)\n  )\n  (classifier): TransformerClassifier(\n    (attention_pool): Linear(in_features=512, out_features=1, bias=True)\n    (dropout): Dropout(p=0.0, inplace=False)\n    (blocks): ModuleList(\n      (0): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): Identity()\n      )\n      (1): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): DropPath()\n      )\n      (2): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): DropPath()\n      )\n      (3): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): DropPath()\n      )\n      (4): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): DropPath()\n      )\n      (5): TransformerEncoder(\n        (pre_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (self_attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=False)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n        (dropout1): Dropout(p=0.0, inplace=False)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n        (dropout2): Dropout(p=0.0, inplace=False)\n        (drop_path): DropPath()\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (fc): Identity()\n  )\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model.to('cuda')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 512, 32, 32]          13,824\n",
      "              ReLU-2          [-1, 512, 32, 32]               0\n",
      "         MaxPool2d-3          [-1, 512, 16, 16]               0\n",
      "           Flatten-4             [-1, 512, 256]               0\n",
      "         Tokenizer-5             [-1, 256, 512]               0\n",
      "           Dropout-6             [-1, 256, 512]               0\n",
      "         LayerNorm-7             [-1, 256, 512]           1,024\n",
      "            Linear-8            [-1, 256, 1536]         786,432\n",
      "           Dropout-9          [-1, 4, 256, 256]               0\n",
      "           Linear-10             [-1, 256, 512]         262,656\n",
      "          Dropout-11             [-1, 256, 512]               0\n",
      "        Attention-12             [-1, 256, 512]               0\n",
      "         Identity-13             [-1, 256, 512]               0\n",
      "        LayerNorm-14             [-1, 256, 512]           1,024\n",
      "           Linear-15            [-1, 256, 1024]         525,312\n",
      "          Dropout-16            [-1, 256, 1024]               0\n",
      "           Linear-17             [-1, 256, 512]         524,800\n",
      "          Dropout-18             [-1, 256, 512]               0\n",
      "         Identity-19             [-1, 256, 512]               0\n",
      "TransformerEncoder-20             [-1, 256, 512]               0\n",
      "        LayerNorm-21             [-1, 256, 512]           1,024\n",
      "           Linear-22            [-1, 256, 1536]         786,432\n",
      "          Dropout-23          [-1, 4, 256, 256]               0\n",
      "           Linear-24             [-1, 256, 512]         262,656\n",
      "          Dropout-25             [-1, 256, 512]               0\n",
      "        Attention-26             [-1, 256, 512]               0\n",
      "         DropPath-27             [-1, 256, 512]               0\n",
      "        LayerNorm-28             [-1, 256, 512]           1,024\n",
      "           Linear-29            [-1, 256, 1024]         525,312\n",
      "          Dropout-30            [-1, 256, 1024]               0\n",
      "           Linear-31             [-1, 256, 512]         524,800\n",
      "          Dropout-32             [-1, 256, 512]               0\n",
      "         DropPath-33             [-1, 256, 512]               0\n",
      "TransformerEncoder-34             [-1, 256, 512]               0\n",
      "        LayerNorm-35             [-1, 256, 512]           1,024\n",
      "           Linear-36            [-1, 256, 1536]         786,432\n",
      "          Dropout-37          [-1, 4, 256, 256]               0\n",
      "           Linear-38             [-1, 256, 512]         262,656\n",
      "          Dropout-39             [-1, 256, 512]               0\n",
      "        Attention-40             [-1, 256, 512]               0\n",
      "         DropPath-41             [-1, 256, 512]               0\n",
      "        LayerNorm-42             [-1, 256, 512]           1,024\n",
      "           Linear-43            [-1, 256, 1024]         525,312\n",
      "          Dropout-44            [-1, 256, 1024]               0\n",
      "           Linear-45             [-1, 256, 512]         524,800\n",
      "          Dropout-46             [-1, 256, 512]               0\n",
      "         DropPath-47             [-1, 256, 512]               0\n",
      "TransformerEncoder-48             [-1, 256, 512]               0\n",
      "        LayerNorm-49             [-1, 256, 512]           1,024\n",
      "           Linear-50            [-1, 256, 1536]         786,432\n",
      "          Dropout-51          [-1, 4, 256, 256]               0\n",
      "           Linear-52             [-1, 256, 512]         262,656\n",
      "          Dropout-53             [-1, 256, 512]               0\n",
      "        Attention-54             [-1, 256, 512]               0\n",
      "         DropPath-55             [-1, 256, 512]               0\n",
      "        LayerNorm-56             [-1, 256, 512]           1,024\n",
      "           Linear-57            [-1, 256, 1024]         525,312\n",
      "          Dropout-58            [-1, 256, 1024]               0\n",
      "           Linear-59             [-1, 256, 512]         524,800\n",
      "          Dropout-60             [-1, 256, 512]               0\n",
      "         DropPath-61             [-1, 256, 512]               0\n",
      "TransformerEncoder-62             [-1, 256, 512]               0\n",
      "        LayerNorm-63             [-1, 256, 512]           1,024\n",
      "           Linear-64            [-1, 256, 1536]         786,432\n",
      "          Dropout-65          [-1, 4, 256, 256]               0\n",
      "           Linear-66             [-1, 256, 512]         262,656\n",
      "          Dropout-67             [-1, 256, 512]               0\n",
      "        Attention-68             [-1, 256, 512]               0\n",
      "         DropPath-69             [-1, 256, 512]               0\n",
      "        LayerNorm-70             [-1, 256, 512]           1,024\n",
      "           Linear-71            [-1, 256, 1024]         525,312\n",
      "          Dropout-72            [-1, 256, 1024]               0\n",
      "           Linear-73             [-1, 256, 512]         524,800\n",
      "          Dropout-74             [-1, 256, 512]               0\n",
      "         DropPath-75             [-1, 256, 512]               0\n",
      "TransformerEncoder-76             [-1, 256, 512]               0\n",
      "        LayerNorm-77             [-1, 256, 512]           1,024\n",
      "           Linear-78            [-1, 256, 1536]         786,432\n",
      "          Dropout-79          [-1, 4, 256, 256]               0\n",
      "           Linear-80             [-1, 256, 512]         262,656\n",
      "          Dropout-81             [-1, 256, 512]               0\n",
      "        Attention-82             [-1, 256, 512]               0\n",
      "         DropPath-83             [-1, 256, 512]               0\n",
      "        LayerNorm-84             [-1, 256, 512]           1,024\n",
      "           Linear-85            [-1, 256, 1024]         525,312\n",
      "          Dropout-86            [-1, 256, 1024]               0\n",
      "           Linear-87             [-1, 256, 512]         524,800\n",
      "          Dropout-88             [-1, 256, 512]               0\n",
      "         DropPath-89             [-1, 256, 512]               0\n",
      "TransformerEncoder-90             [-1, 256, 512]               0\n",
      "        LayerNorm-91             [-1, 256, 512]           1,024\n",
      "           Linear-92               [-1, 256, 1]             513\n",
      "         Identity-93                  [-1, 512]               0\n",
      "TransformerClassifier-94                  [-1, 512]               0\n",
      "================================================================\n",
      "Total params: 12,622,849\n",
      "Trainable params: 12,622,849\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 127.01\n",
      "Params size (MB): 48.15\n",
      "Estimated Total Size (MB): 175.17\n",
      "----------------------------------------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "summary(model,(3,32,32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}